{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,3\"\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "from model.lstm_pm import LSTM_PM\n",
    "from data.handpose_data import UCIHandPoseDataset\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import scipy.misc\n",
    "from IPython.core.debugger import set_trace\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_temporal = 4\n",
    "nb_epochs=20\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "data_dir = 'dataset/frames/combine_hand/combine_hand'\n",
    "label_dir = 'dataset/label/combine_label/combine_label'\n",
    "dataset = UCIHandPoseDataset(data_dir=data_dir, label_dir=label_dir, temporal=nb_temporal)\n",
    "train_dataset = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "net = LSTM_PM(T=nb_temporal).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_history_init():\n",
    "    loss_history={}\n",
    "    for i in range(batch_size):\n",
    "        loss_history['train'+str(i+1)]=[]\n",
    "        loss_history['test'+str(i+1)] = []\n",
    "    loss_history['train'] = []\n",
    "    loss_history['test'] = []\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(save_path, nb_temporal, pred_heatmap, label_map=None, size=45, nb_heatmap=22):\n",
    "    # predict_heatmaps shape: [nb_temporal, batch_size,22,45,45]\n",
    "    for i in range(pred_heatmap[0].shape[0]):# each batch (person)\n",
    "        if label_map==None:\n",
    "            s = np.zeros((size, size*nb_temporal)) \n",
    "        else:\n",
    "            s = np.zeros((size*2, size*nb_temporal)) \n",
    "            \n",
    "        for j in range(len(pred_heatmap)):  # each temporal\n",
    "            for k in range(nb_heatmap):\n",
    "                if label_map==None:\n",
    "                    s[:, j*45:(j+1)*45]+=predict_heatmaps[j].cpu().data.numpy()[i,k,:,:]\n",
    "                else:\n",
    "                    s[:45, j*45:(j+1)*45]+=label_map[i , j, k, :, :]\n",
    "                    s[45:, j*45:(j+1)*45]+=predict_heatmaps[j].cpu().data.numpy()[i,k,:,:]\n",
    "        scipy.misc.imsave(save_path+str(i+1)+'.jpg', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = loss_history_init()\n",
    "save_json_path = \"train_history.json\"\n",
    "save_weight_path = \"./checkpoint/penn_lstm_pm_\"\n",
    "save_runtime_heatmap_path = \"runtime_heatmaps/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch......................1\n",
      "Epoch: 1 itr: 1 loss: 0.00296654156409\n",
      "Epoch: 1 itr: 2 loss: 0.00290113128722\n",
      "Epoch: 1 itr: 3 loss: 0.00232817674987\n",
      "Epoch: 1 itr: 4 loss: 0.0023020291701\n",
      "Epoch: 1 itr: 5 loss: 0.00266363937408\n",
      "Epoch: 1 itr: 6 loss: 0.00271297967993\n",
      "Epoch: 1 itr: 7 loss: 0.00241631129757\n",
      "Epoch: 1 itr: 8 loss: 0.00215244945139\n",
      "Epoch: 1 itr: 9 loss: 0.00286916038021\n",
      "Epoch: 1 itr: 10 loss: 0.0024396693334\n",
      "Epoch: 1 itr: 11 loss: 0.00239616958424\n",
      "epoch......................2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danningx/.conda/envs/handpose-py2/lib/python2.7/site-packages/ipykernel_launcher.py:65: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 itr: 1 loss: 0.00315680145286\n",
      "Epoch: 2 itr: 2 loss: 0.00303068198264\n",
      "Epoch: 2 itr: 3 loss: 0.00297151925042\n",
      "Epoch: 2 itr: 4 loss: 0.00307324924506\n",
      "Epoch: 2 itr: 5 loss: 0.00294928648509\n",
      "Epoch: 2 itr: 6 loss: 0.00329540669918\n",
      "Epoch: 2 itr: 7 loss: 0.00295939738862\n",
      "Epoch: 2 itr: 8 loss: 0.00333702214994\n",
      "Epoch: 2 itr: 9 loss: 0.00386966415681\n",
      "Epoch: 2 itr: 10 loss: 0.00273666065186\n",
      "Epoch: 2 itr: 11 loss: 0.00290952716023\n",
      "epoch......................3\n",
      "Epoch: 3 itr: 1 loss: 0.00331067317165\n",
      "Epoch: 3 itr: 2 loss: 0.0028154538013\n",
      "Epoch: 3 itr: 3 loss: 0.00275068939663\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(params=net.parameters(), lr=8e-6, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=40000, gamma=0.333)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    print 'epoch......................' + str(epoch+1)\n",
    "    net.train(True)\n",
    "    for idx, (tmp1, tmp2, center_map) in enumerate(train_dataset):\n",
    "        if tmp2.shape[0]!=batch_size:\n",
    "            break           \n",
    "        images = tmp1\n",
    "        label_map = tmp2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predict_heatmaps = net(images, center_map)  # list\n",
    "        train_loss = 0\n",
    "        for i in range(len(predict_heatmaps)):\n",
    "            predict = predict_heatmaps[i]# each batch(person)\n",
    "            target = label_map[:,i, :, :, :]\n",
    "            tmp_loss = criterion(predict, target) #loss of each temporal\n",
    "            loss_history['train'+str(i+1)].append(float(tmp_loss))\n",
    "            train_loss += tmp_loss * batch_size\n",
    "\n",
    "        loss_history['train'].append(float(loss))\n",
    "        train_loss.backward()\n",
    "        ## *******************************************\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if idx%10==9:\n",
    "            print \"Epoch: \"+str(epoch+1)+\" itr: \"+str(idx+1)+\" loss: \"+str(float(loss))\n",
    "        \n",
    "#         if idx==10:\n",
    "#             break\n",
    "    #save loss history and parameters after each epoch\n",
    "    json.dump(loss_history, open(save_json_path, 'wb')) \n",
    "    \n",
    "    if epoch%50==0: #save parameters\n",
    "        torch.save(net.state_dict(),save_weight_path+str(epoch))\n",
    "    \n",
    "    #***********************save train heatmap after each 20 epoch**********************************\n",
    "    if epoch%20 == 0:\n",
    "        tmp_path = save_runtime_heatmap_path+'/'+'train'+'/e'+str(epoch)+'_bat'\n",
    "        save_image(tmp_path, nb_temporal, predict_heatmaps, label_map)\n",
    "    \n",
    "\n",
    "    #*****************************test after each epoch *****************************************\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    for stp in range(int(len(x.shape[0])/batch_size)+1):\n",
    "        #load test data\n",
    "    \n",
    "        predict_heatmaps = net(testx_batch, center_map_test)\n",
    "        for i in range(len(predict_heatmaps)):\n",
    "            predict = predict_heatmaps[i]# each batch(person)\n",
    "            target = label_map[:,i, :, :, :]\n",
    "            # print target.shape\n",
    "            tmp_loss = criterion(predict, target)\n",
    "            loss_history['test'+str(i+1)].append(tmp_loss)\n",
    "            test_loss += tmp_loss * testy_batch.shape[0]          \n",
    "        loss_history['test'].append(test_loss)\n",
    "        \n",
    "        if epoch%20 == 0:  # save image\n",
    "            tmp_path = save_runtime_heatmap_path+'/'+'test'+'/e'+str(epoch)+'_bat'\n",
    "            save_image(tmp_path, nb_temporal, predict_heatmaps, label_map)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handpose-py2(myenv)",
   "language": "python",
   "name": "handpose-py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
